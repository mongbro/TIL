{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9. news",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KadzPLumXjCarXJpJHeEC51E8MyDCEY-",
      "authorship_tag": "ABX9TyNo+NJ+3Iju8UkGBrd7hLAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mongbro/TIL/blob/master/9_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppGL1OawacB"
      },
      "source": [
        "### keras RNN으로 BBC 기사 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaoW5i0whKR"
      },
      "source": [
        "1. 패키지 수입 및 파라미터 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM3blB2GcaBL"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import numpy as np\r\n",
        "import csv\r\n",
        "import nltk # natural language tool kit\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM, Dropout, Embedding\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from time import time\r\n",
        "from sklearn.metrics import confusion_matrix, f1_score\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZGC6qZ_yaUr"
      },
      "source": [
        "# 파라미터 지정\r\n",
        "MY_VOCAB = 5000   # 내가 사용할 단어의 수, 제일 많이 사용된 단어\r\n",
        "MY_EMBED = 64     # 임베딩 차원\r\n",
        "MY_HIDDEN = 100   # LSTM 셀의 규모\r\n",
        "MY_LEN = 200      # 기사의 길이\r\n",
        "MY_SPLIT = 0.8    # 학습용 데이터의 비율\r\n",
        "MY_SAMPLE = 0   # 샘플 기사\r\n",
        "MY_EPOCH = 100  # 반복 학습 수\r\n",
        "TRAIN_MODE = 1    # 학습 모드와 평가 모드 선택"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFTyPS7C0d2Y"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrpUXHot0gD3",
        "outputId": "33464f96-c306-49b9-9421-1649ccc1c1d8"
      },
      "source": [
        "# 제외어 (stopword) 설정\r\n",
        "nltk.download('stopwords')\r\n",
        "MY_STOP = set(nltk.corpus.stopwords.words('english'))\r\n",
        "\r\n",
        "print('영어 단어 제외')\r\n",
        "print(MY_STOP)\r\n",
        "print('제외어 개수 :', len(MY_STOP))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "영어 단어 제외\n",
            "{'them', 're', 'for', 'those', 'do', 'out', \"mustn't\", 'needn', 'and', 'before', 'doesn', \"wouldn't\", 'which', 'wasn', 'theirs', 'as', 'not', 'are', 'yourself', 'very', 'shouldn', 'aren', 'after', 'where', 'themselves', 'm', 'any', 'these', 've', 'both', 'mightn', 'herself', 'between', 'were', 'but', 'our', 'than', 'what', 'hers', 'he', 'their', 'how', 'll', 'this', \"needn't\", 'has', 'because', 'too', \"doesn't\", 'no', 'why', 'against', 'above', \"mightn't\", \"shan't\", 'did', 'to', \"wasn't\", \"it's\", \"don't\", 'whom', 'she', 'through', 'an', 'most', 'y', 'during', 'once', \"won't\", 'hadn', \"hadn't\", 'into', 'then', 'other', 'a', 'or', 'down', 'itself', 'its', 'hasn', 'each', 'can', 'there', \"you'd\", \"you're\", 'while', 'being', 'should', 'of', 't', 'about', 'had', 'doing', 'was', 'my', 'have', 'until', 'the', \"you'll\", \"aren't\", 'own', 'isn', 'few', 'yourselves', 'at', 'with', 'only', 'having', 'up', 'will', 'ma', 'd', \"shouldn't\", 'they', 'his', 'off', 'who', 'didn', 'further', 'me', 'you', \"isn't\", 'myself', 'all', 'haven', \"weren't\", 'ours', \"that'll\", 'again', 's', 'over', 'i', \"haven't\", 'her', 'now', 'o', 'shan', \"you've\", 'your', \"she's\", 'in', 'don', 'if', 'weren', 'wouldn', 'just', 'couldn', 'more', 'be', 'himself', 'ourselves', 'we', 'does', 'such', 'below', 'here', 'been', \"should've\", 'am', 'on', 'nor', 'it', 'him', 'is', 'from', 'same', \"hasn't\", 'won', 'when', 'so', 'ain', \"didn't\", \"couldn't\", 'under', 'that', 'by', 'mustn', 'yours', 'some'}\n",
            "제외어 개수 : 179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAWYofJ12gxH"
      },
      "source": [
        "# 데이터 보관 창고\r\n",
        "original = []\r\n",
        "articles = []\r\n",
        "labels = []"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLHVz1oR262X",
        "outputId": "327efd64-edd0-4680-9644-9492758a9451"
      },
      "source": [
        "# BBC 파일 읽고 처리\r\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/bbc-text.csv', 'r') as file:\r\n",
        "    # 칼럼 이름 읽기\r\n",
        "    reader = csv.reader(file)\r\n",
        "    next(reader)\r\n",
        "\r\n",
        "    # 기사 하나씩 처리\r\n",
        "    for row in reader:\r\n",
        "        # 카테고리 저장\r\n",
        "        labels.append(row[0])\r\n",
        "\r\n",
        "        # 원본 기사 저장\r\n",
        "        original.append(row[1])\r\n",
        "\r\n",
        "        # 제외어 삭제 하기\r\n",
        "        news = row[1]\r\n",
        "        for word in MY_STOP:\r\n",
        "            mask = ' ' + word + ' '\r\n",
        "            news = news.replace(mask, ' ')\r\n",
        "\r\n",
        "        # 제외어를 뺀 기사 저장\r\n",
        "        articles.append(news)\r\n",
        "        \r\n",
        "print('처리한 기사 수 :', len(articles))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리한 기사 수 : 2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8uuXJCABh6K",
        "outputId": "cd3d6cb8-7e33-40ed-fff0-ac765ab0dfd9"
      },
      "source": [
        "# 샘플 기사 출력\r\n",
        "print('샘플 기사 원본 >> ')\r\n",
        "print(original[MY_SAMPLE])\r\n",
        "print(labels[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(original[MY_SAMPLE].split()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 원본 >> \n",
            "tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.\n",
            "tech\n",
            "총 단어 수 : 737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd0JmTafEVrq",
        "outputId": "260fa915-2881-4e76-9ad8-f94c1f02798e"
      },
      "source": [
        "# 제외어 처리 결과\r\n",
        "print('샘플 기사 제외어 삭제본 >> ')\r\n",
        "print(articles[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(articles[MY_SAMPLE].split()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 제외어 삭제본 >> \n",
            "tv future hands viewers home theatre systems  plasma high-definition tvs  digital video recorders moving living room  way people watch tv radically different five years  time.  according expert panel gathered annual consumer electronics show las vegas discuss new technologies impact one favourite pastimes. us leading trend  programmes content delivered viewers via home networks  cable  satellite  telecoms companies  broadband service providers front rooms portable devices.  one talked-about technologies ces digital personal video recorders (dvr pvr). set-top boxes  like us tivo uk sky+ system  allow people record  store  play  pause forward wind tv programmes want.  essentially  technology allows much personalised tv. also built-in high-definition tv sets  big business japan us  slower take europe lack high-definition programming. people forward wind adverts  also forget abiding network channel schedules  putting together a-la-carte entertainment. us networks cable satellite companies worried means terms advertising revenues well  brand identity  viewer loyalty channels. although us leads technology moment  also concern raised europe  particularly growing uptake services like sky+.  happens today  see nine months years  time uk   adam hume  bbc broadcast futurologist told bbc news website. likes bbc  issues lost advertising revenue yet. pressing issue moment commercial uk broadcasters  brand loyalty important everyone.  talking content brands rather network brands   said tim hanlon  brand communications firm starcom mediavest.  reality broadband connections  anybody producer content.  added:  challenge hard promote programme much choice.   means  said stacey jolna  senior vice president tv guide tv group  way people find content want watch simplified tv viewers. means networks  us terms  channels could take leaf google book search engine future  instead scheduler help people find want watch. kind channel model might work younger ipod generation used taking control gadgets play them. might suit everyone  panel recognised. older generations comfortable familiar schedules channel brands know getting. perhaps want much choice put hands  mr hanlon suggested.  end  kids diapers pushing buttons already - everything possible available   said mr hanlon.  ultimately  consumer tell market want.   50 000 new gadgets technologies showcased ces  many enhancing tv-watching experience. high-definition tv sets everywhere many new models lcd (liquid crystal display) tvs launched dvr capability built  instead external boxes. one example launched show humax 26-inch lcd tv 80-hour tivo dvr dvd recorder. one us biggest satellite tv companies  directtv  even launched branded dvr show 100-hours recording capability  instant replay  search function. set pause rewind tv 90 hours. microsoft chief bill gates announced pre-show keynote speech partnership tivo  called tivotogo  means people play recorded programmes windows pcs mobile devices. reflect increasing trend freeing multimedia people watch want  want.\n",
            "총 단어 수 : 412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyXn1KQnGEaR",
        "outputId": "a986d5a6-1023-4a02-e0e5-bb75beb46eeb"
      },
      "source": [
        "# Tokenizer 처리\r\n",
        "A_token = Tokenizer(num_words = MY_VOCAB,\r\n",
        "                    oov_token = 'oov')\r\n",
        "# oov란? 제외되지 않은 단어 중에서 사용 빈도가 적어서 5000개 단어에 포함하지 않는 단어들\r\n",
        "#                        MY_VOCAB가 적어질수록 oov가 늘어난다\r\n",
        "\r\n",
        "A_token.fit_on_texts(articles)\r\n",
        "A_tokenized = A_token.texts_to_sequences(articles)  # => 텍스트를 숫자로 변환(hash function)\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(A_token.sequences_to_texts([[1]]))      # 1은 어떤 단어인가? => 'oov'(생략된 단어)\r\n",
        "                                              # MY_VOCAB가 적어질수록 1이 늘어난다\r\n",
        "print(A_token.sequences_to_texts([[1140]]))   # 1140은 어떤 단어인가? => 'the'\r\n",
        "print(A_token.texts_to_sequences(['the']))    # 'the'는 어떤 숫자인가? => 1140\r\n",
        "print(A_token.texts_to_sequences(['oov']))    # 'the'는 어떤 숫자인가? => 1140"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['oov']\n",
            "['present']\n",
            "[[1173]]\n",
            "[[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPJApClH6Q4",
        "outputId": "0a049997-021b-485a-e984-f6c4a105e3c3"
      },
      "source": [
        "# Token  처리 결과 출력\r\n",
        "sample = A_tokenized[MY_SAMPLE]\r\n",
        "print(sample)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[88, 165, 1143, 1206, 48, 1108, 727, 1, 77, 1060, 4252, 137, 173, 4113, 1331, 1297, 1583, 41, 7, 935, 88, 1, 316, 84, 19, 14, 130, 3115, 1317, 2507, 562, 406, 1263, 65, 2949, 3033, 1743, 8, 880, 740, 10, 940, 1, 9, 641, 1566, 1039, 401, 1986, 1206, 763, 48, 488, 1485, 2102, 1642, 125, 320, 114, 2731, 803, 1, 1074, 595, 10, 4399, 3833, 880, 2566, 137, 338, 173, 4113, 1, 1, 38, 66, 3204, 25, 9, 1, 18, 1383, 135, 441, 7, 128, 1384, 74, 4583, 474, 1, 88, 1039, 79, 1, 75, 2103, 56, 1, 88, 6, 1109, 606, 77, 1060, 88, 1956, 138, 149, 407, 9, 2865, 40, 139, 1207, 77, 1060, 4400, 7, 474, 1, 3116, 6, 2680, 1, 399, 1083, 1, 1362, 602, 1385, 2066, 1, 741, 9, 488, 1485, 2102, 125, 1904, 397, 881, 2067, 1608, 37, 1807, 2567, 4983, 1, 2508, 238, 9, 2621, 75, 804, 6, 1075, 1119, 139, 783, 563, 1, 126, 25, 1383, 1808, 432, 82, 941, 109, 19, 14, 18, 3382, 1, 36, 1442, 1, 22, 36, 91, 349, 2381, 36, 451, 230, 2067, 1363, 328, 1, 313, 804, 1120, 18, 2622, 1807, 1, 284, 721, 1162, 401, 2029, 387, 399, 2029, 2, 1298, 1, 1807, 1841, 63, 1, 1, 1782, 320, 1809, 3383, 1187, 401, 42, 843, 257, 2950, 353, 56, 557, 397, 2, 1, 1, 656, 1299, 194, 88, 3978, 88, 97, 41, 7, 339, 401, 79, 935, 1, 88, 1206, 397, 488, 9, 881, 2508, 11, 40, 1, 837, 532, 390, 1842, 165, 558, 1, 131, 7, 339, 79, 935, 1188, 1083, 1486, 355, 61, 2030, 1249, 772, 86, 249, 286, 1017, 74, 598, 355, 2568, 721, 1317, 2288, 1657, 1, 3834, 3205, 1, 1083, 2029, 174, 382, 1544, 79, 56, 557, 106, 1143, 3, 1, 961, 92, 3206, 1, 2335, 1, 105, 754, 422, 427, 2, 3, 1, 2623, 406, 1487, 43, 79, 412, 32, 8, 1017, 880, 1, 2566, 30, 4253, 88, 1711, 838, 77, 1060, 88, 1956, 1, 30, 8, 1505, 1, 1, 4114, 1458, 4252, 633, 1, 3835, 1109, 558, 4115, 3204, 10, 897, 633, 65, 1, 1332, 1, 1, 88, 1459, 993, 1, 1, 573, 4116, 10, 9, 235, 2102, 88, 125, 1, 98, 633, 3034, 1, 65, 486, 697, 2463, 3835, 1, 3501, 390, 4781, 38, 4583, 1, 88, 1765, 697, 283, 113, 274, 2795, 331, 642, 65, 3979, 747, 3724, 1, 152, 1, 397, 7, 74, 1766, 1039, 810, 860, 81, 595, 3117, 1364, 1566, 1, 1658, 7, 935, 79, 79]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-nD_bkAPlaz",
        "outputId": "6278c4c0-a1ac-4cda-a7f9-c2ed79ff4754"
      },
      "source": [
        "# 기사 통계 내기\r\n",
        "# 제외어 빼고 제일 긴, 짧은 기사 구하기\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "print('제일 짧은 기사 :', shortest)\r\n",
        "\r\n",
        "# 모든 기사에서 제외어를 빼고 사용된 모든 단어 수\r\n",
        "print('총 단어 수 :', len(A_token.word_counts))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 2279\n",
            "제일 짧은 기사 : 50\n",
            "총 단어 수 : 29698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZENVw3UR4Hb",
        "outputId": "88a5f5c2-6661-41de-ecb3-936d32cf8175"
      },
      "source": [
        "# 기사 길이 맞추기\r\n",
        "# MY_LEN보다 긴건 자르고 짧은건 무언가(0)를 더해준다\r\n",
        "A_tokenized = pad_sequences(A_tokenized,\r\n",
        "                            maxlen = MY_LEN,\r\n",
        "                            padding = 'post',     # 200단어보다 짧은 기사는 뒷부분을 0으로 패딩처리\r\n",
        "                            truncating = 'post')  # 200단어보다 긴 기사는 뒷부분 삭제\r\n",
        "\r\n",
        "# 기사 길이 확인\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "print('제일 짧은 기사 :', shortest)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 200\n",
            "제일 짧은 기사 : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0xjopnMUJ-E",
        "outputId": "625fb859-cb99-4169-b238-ff283d451421"
      },
      "source": [
        "# 라벨 tokenization\r\n",
        "C_token = Tokenizer()\r\n",
        "C_token.fit_on_texts(labels)\r\n",
        "C_tokenized = C_token.texts_to_sequences(labels)\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(C_token.word_index)\r\n",
        "print(C_tokenized)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgCKe-a8WePz",
        "outputId": "39d08aa6-15fd-487d-9018-69e00e76bce6"
      },
      "source": [
        "# 데이터 4분할\r\n",
        "C_tokenized = np.array(C_tokenized)   # 기존의 C_tokenized는 list형식이다\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized,\r\n",
        "                                                    C_tokenized,\r\n",
        "                                                    train_size = MY_SPLIT,\r\n",
        "                                                    shuffle = False)\r\n",
        "\r\n",
        "# 데이터 모양 확인\r\n",
        "print('학습용 입력 데이터 모양 :', X_train.shape)\r\n",
        "print('학습용 출력 데이터 모양 :', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터 모양 :', X_test.shape)\r\n",
        "print('평가용 출력 데이터 모양 :', Y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 모양 : (1780, 200)\n",
            "학습용 출력 데이터 모양 : (1780, 1)\n",
            "평가용 입력 데이터 모양 : (445, 200)\n",
            "평가용 출력 데이터 모양 : (445, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EarbCbaTejNU"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEkrvtRXehTl",
        "outputId": "c3fbfd83-3b62-4eb6-ea1a-82756df16f00"
      },
      "source": [
        "# RNN 구현\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(input_dim = MY_VOCAB,       # 1 * 5000 행렬에 5000 * 64행렬을 곱해서\r\n",
        "                    output_dim = MY_EMBED))     # 1 * 64 행렬로 만든다.\r\n",
        "\r\n",
        "model.add(Dropout(rate = 0.5))  # 임의의 뉴런의 출력을 일부러 0으로 만드는 작업\r\n",
        "                                # 왜? => 과적합을 방지하기 위해서\r\n",
        "\r\n",
        "model.add(Bidirectional(LSTM(units = MY_HIDDEN)))\r\n",
        "\r\n",
        "model.add(Dense(units = 6,                  # 왜 5가 아니라 6일까??\r\n",
        "                activation = 'softmax'))    # 아까 output은 1~5였는데 RNN에서 맨 처음은 0이라서\r\n",
        "                                            # units를 5로 하면 0~4 까지만 검색을 한다.\r\n",
        "                                            # 즉 5번이 나올 수 없다.\r\n",
        "print('RNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN 요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200)               132000    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 453,206\n",
            "Trainable params: 453,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqc7M_MQqf6B"
      },
      "source": [
        "4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWzQhWIeqhbp",
        "outputId": "d4de770e-ddba-47d7-f33d-63d1955ceaeb"
      },
      "source": [
        "# RNN 학습\r\n",
        "model.compile(optimizer = 'adam',\r\n",
        "              loss = 'sparse_categorical_crossentropy',\r\n",
        "              metrics = ['acc'])\r\n",
        "\r\n",
        "print('학습 시작')\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x = X_train,\r\n",
        "          y = Y_train,\r\n",
        "          epochs = MY_EPOCH,\r\n",
        "          verbose = 1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습시간 : {:.2f}초'.format(end - begin))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 시작\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 10s 24ms/step - loss: 1.6669 - acc: 0.2539\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.3121 - acc: 0.4571\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.7335 - acc: 0.7835\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5618 - acc: 0.8125\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4696 - acc: 0.8558\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2113 - acc: 0.9534\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0945 - acc: 0.9807\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0834 - acc: 0.9787\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0605 - acc: 0.9805\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1375 - acc: 0.9673\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0334 - acc: 0.9967\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0572 - acc: 0.9947\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0521 - acc: 0.9990\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0186 - acc: 0.9961\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0093 - acc: 0.9975\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0128 - acc: 0.9975\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 8.1112e-04 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.1289e-04 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 5.3369e-04 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.2675e-04 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.0256e-04 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.1217e-04 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.3610e-04 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.8139e-04 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.7822e-04 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.4934e-04 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.8117e-04 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.0350e-04 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0328 - acc: 0.9868\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0074 - acc: 0.9994\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0072 - acc: 0.9989\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0153 - acc: 0.9960\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.6961e-04 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 6.6663e-04 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.5960e-04 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.7902e-04 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.3609e-04 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.8348e-04 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.4760e-04 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.1545e-04 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0033 - acc: 0.9990\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.6248e-04 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.1092e-04 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 9.1883e-04 - acc: 0.9995\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.1542e-04 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.9300e-04 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.5457e-04 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0041 - acc: 0.9988\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.3348e-04 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.8813e-04 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 2.3173e-04 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.0916e-04 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.8262e-04 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.6319e-04 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.3840e-04 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.3190e-04 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.0828e-04 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.0651e-04 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.0483e-04 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.0368e-04 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 9.0492e-05 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.8449e-05 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 7.5673e-05 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.4451e-05 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.6736e-05 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.1361e-05 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.3190e-05 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.4250e-05 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.7156e-05 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.9132e-05 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 5.2279e-05 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 5.2954e-05 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.5358e-05 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.8090e-05 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.2828e-05 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.2226e-05 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 4.2212e-05 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.8848e-05 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.9748e-05 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 3.6709e-05 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.3738e-05 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 3.3391e-05 - acc: 1.0000\n",
            "학습시간 : 130.05초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fefJp5XnsjYS"
      },
      "source": [
        "5. 인공 신경망 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lGIzRhGslVg",
        "outputId": "b29fb056-89a5-44b0-eb7f-cbdf6e3a3e41"
      },
      "source": [
        "# RNN 평가\r\n",
        "score = model.evaluate(X_test, Y_test,\r\n",
        "                       verbose = 0)\r\n",
        "\r\n",
        "print('최종 손실값 :', score[0])\r\n",
        "print('최종 정확도 :', score[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 손실값 : 0.27695608139038086\n",
            "최종 정확도 : 0.9528089761734009\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}